{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset,DatasetDict,load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from itertools import islice\n",
    "import copy\n",
    "from torchmetrics.retrieval import RetrievalMAP, RetrievalPrecision\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1136a5090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define main path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/Users/aakashagarwal/Downloads/ir_assignment2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = \"evanarlian/imagenet_1k_resized_256\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c10bb79c47241bbbcbf2bec988ebd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12436b633354c9ea952548dd617f95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3293f087ec0c426a965971d8bcdce6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(mydataset)\n",
    "ds = ds.rename_column(\"image\", \"img\")\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': ds['train'],\n",
    "    'test': ds['val']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "def add_index(example, idx):\n",
    "    example['index'] = idx\n",
    "    return example\n",
    "\n",
    "# Define the number of shards\n",
    "num_shards = 10  # Adjust based on memory constraints and dataset size\n",
    "batch_size = 1000  # Adjust this based on memory limits\n",
    "\n",
    "# List to hold processed shard datasets\n",
    "processed_shards = {split: [] for split in ['train', 'test']}\n",
    "\n",
    "# Loop through each split and each shard\n",
    "for split in ['train', 'test']:\n",
    "    # Calculate shard size\n",
    "    shard_size = len(ds[split]) // num_shards\n",
    "\n",
    "    for shard in range(num_shards):\n",
    "        # Slice the dataset for the current shard\n",
    "        shard_start = shard * shard_size\n",
    "        shard_end = shard_start + shard_size\n",
    "\n",
    "        if shard == num_shards - 1:  # Ensure the last shard includes any remaining examples\n",
    "            shard_end = len(ds[split])\n",
    "\n",
    "        # Apply the map function to the current shard\n",
    "        shard_ds = ds[split].select(range(shard_start, shard_end))\n",
    "        shard_ds = shard_ds.map(add_index, with_indices=True, batched=True, batch_size=batch_size)\n",
    "\n",
    "        # Append the processed shard to the list\n",
    "        processed_shards[split].append(shard_ds)\n",
    "\n",
    "    # Concatenate all processed shards for the current split\n",
    "    ds[split] = concatenate_datasets(processed_shards[split])\n",
    "\n",
    "    # Optionally, save the concatenated split to disk if needed\n",
    "    # ds_feature[split].save_to_disk(f\"{split}_processed\")\n",
    "\n",
    "# After the loop, ds_feature will contain the fully processed 'train' and 'test' splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=341x256>,\n",
       " 'label': 77,\n",
       " 'index': 100000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-large-patch32-384\",use_fast=True)\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-large-patch32-384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((384, 384)),  # Resize images to a consistent size\n",
    "        transforms.ToTensor(),           # Convert PIL images to tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Define a function to process and extract features from each image\n",
    "def extract_features(batch):\n",
    "    imgs = batch['img']  # This will now be a list of images in a batch\n",
    "\n",
    "    # Preprocess each image in the batch\n",
    "    img_tensors = [transform(img).unsqueeze(0).to(device) for img in imgs]  # Add batch dimension and move to device\n",
    "    img_tensors = torch.cat(img_tensors, dim=0)  # Concatenate to create a batch tensor\n",
    "\n",
    "    inputs = {'pixel_values': img_tensors}  # Adjust for model input (assuming using Hugging Face models)\n",
    "\n",
    "    # Forward pass through the model to get features\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = outputs.hidden_states  # List of all hidden states\n",
    "\n",
    "    # Extract the final hidden state\n",
    "    final_hidden_state = hidden_states[-1]  # Shape: [batch_size, num_patches, hidden_size]\n",
    "    \n",
    "    # Optionally, pool the features\n",
    "    pooled_features = final_hidden_state.mean(dim=1)  # Shape: [batch_size, hidden_size]\n",
    "    \n",
    "    # Return the features as a new column for the batch\n",
    "    return {\"features\": pooled_features.cpu().numpy()}  # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create features of train data or load from saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features file already exists at /Users/aakashagarwal/Downloads/ir_assignment2/features/imagenet1k/vit_large/. Loading features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51050af55e8546b795bc3a3c90968732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "output_dir = main_path+ f\"features/imagenet1k/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"vit_large/\")  # Saving in Arrow format\n",
    "\n",
    "# Check if features already exist\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"Features file already exists at {output_file}. Loading features...\")\n",
    "    # Load existing features\n",
    "    ds_feature = load_from_disk(output_file)\n",
    "else:\n",
    "    print(\"Features file does not exist. Extracting features...\")\n",
    "\n",
    "    # Use `map` to apply the feature extraction across the entire dataset\n",
    "    ds_feature = ds.map(extract_features, batched=True, batch_size=64)\n",
    "\n",
    "    # Save the dataset with the new features to disk\n",
    "    ds_feature.save_to_disk(output_file)  # Saving in the Arrow format\n",
    "    print(\"Features saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['features', 'label', 'index'],\n",
       "        num_rows: 1281167\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['features', 'label', 'index'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_feature = ds_feature.rename_column(\"labels\", \"label\")\n",
    "ds_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Define the transformations you want to apply to the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to a consistent size\n",
    "        transforms.ToTensor(),           # Convert PIL images to tensors\n",
    "    ])\n",
    "\n",
    "    # Extracting each field from the batch\n",
    "    index = [item['index'] for item in batch]\n",
    "    # images = [transform(item['img']) for item in batch]  # Load and transform images\n",
    "    labels = torch.tensor([item['label'] for item in batch])          # Convert labels to a tensor\n",
    "    features = [torch.tensor(item['features']) for item in batch]     # Convert features to tensors\n",
    "    \n",
    "    # Return a dictionary with batched data\n",
    "    return {\n",
    "        'index': index,\n",
    "        # 'img': torch.stack(images),    # Stack image tensors into a single tensor\n",
    "        'label': labels,\n",
    "        'features': torch.stack(features),  # Stack feature tensors into a single tensor\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# Define the subset sizes\n",
    "# train_subset_size = 50000\n",
    "# test_subset_size = 10000\n",
    "\n",
    "# # Create indices for the subsets\n",
    "# train_indices = list(range(train_subset_size))\n",
    "# test_indices = list(range(test_subset_size))\n",
    "\n",
    "# # Create subsets using the indices\n",
    "# train_subset = Subset(ds_feature['train'], train_indices)\n",
    "# test_subset = Subset(ds_feature['test'], test_indices)\n",
    "\n",
    "# Create dataloaders for the subsets\n",
    "train_dataloader = DataLoader(ds_feature['train'], batch_size=1000, shuffle=False, collate_fn=custom_collate)\n",
    "test_dataloader = DataLoader(ds_feature['test'], batch_size=64, shuffle=False, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ds_feature['train'][1:3]['features']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare a list to store cosine similarities\n",
    "cosine_similarities = []\n",
    "\n",
    "# Function to compute cosine similarities for a single test example\n",
    "def add_cosine_similarities(example):\n",
    "    test_features = example['features']  # Assuming 'features' key holds the features\n",
    "    # Convert to tensor\n",
    "    test_tensor = torch.tensor(test_features).detach().clone()\n",
    "    \n",
    "    # Iterate through the training batches\n",
    "    for train_batch in tqdm(train_dataloader):\n",
    "        train_features = train_batch['features']  # Assuming 'features' key holds the features\n",
    "        \n",
    "        # Convert to tensor\n",
    "        train_tensor = torch.tensor(train_features).detach().clone()\n",
    "        \n",
    "        # Compute cosine similarities between the current test batch and current train batch\n",
    "        cosine = torch.matmul(test_tensor, train_tensor.T)\n",
    "        example['cosine_similarities'] = cosine\n",
    "    \n",
    "    return example  # Return the modified example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each test sample, using with_indices to track batch position\n",
    "ds_feature['test'] = ds_feature['test'].map(add_cosine_similarities, batched=True, batch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Hyperlane Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hash hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_length = 14\n",
    "num_hash_table = 5\n",
    "feature_length = len(ds_feature['train'][0]['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random hyperplanes function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hyperplane(num_hash_table,hash_length,feature_length):\n",
    "    hyperplanes = []\n",
    "    \n",
    "    for i in range(num_hash_table):\n",
    "        hyperplane = (torch.randn(feature_length,hash_length) - 0.5).to(device)\n",
    "        hyperplanes.append(hyperplane)\n",
    "    \n",
    "    return hyperplanes\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create random hyperplanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 14])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperplanes = random_hyperplane(num_hash_table,hash_length,feature_length)\n",
    "hyperplanes[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hash Tables for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 20019/20019 [10:15<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 20019/20019 [09:44<00:00, 34.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 20019/20019 [10:13<00:00, 32.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 20019/20019 [10:24<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 20019/20019 [09:23<00:00, 35.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hash_dict_list = []\n",
    "# Iterate through the dataset\n",
    "for i in range(len(hyperplanes)):\n",
    "    hash_dict = dict()\n",
    "    for batch_idx, data in enumerate(tqdm(train_dataloader, desc=\"Processing Batches\")):\n",
    "        \n",
    "        # Extract features and compute hash code\n",
    "        feature = data['features'].to(device)\n",
    "\n",
    "        hash_value = (feature @ hyperplanes[i])\n",
    "        hash_code = torch.where(hash_value > 0, 1, 0)\n",
    "        \n",
    "        # Convert the hash code tensor to a list of tuples for dictionary key compatibility\n",
    "        hash_code_list = hash_code.tolist()\n",
    "\n",
    "        # Iterate through each image and label in the batch\n",
    "        for j in range(len(hash_code_list)):\n",
    "            # Create a unique key from the hash code of each image\n",
    "            hash_code_key = tuple(hash_code_list[j])\n",
    "            \n",
    "            # Initialize the list if this hash code key is not yet in the dictionary\n",
    "            if hash_code_key not in hash_dict:\n",
    "                hash_dict[hash_code_key] = []\n",
    "            \n",
    "            # Append the tuple (batch index, image index, label) to the list of this hash code key\n",
    "            hash_dict[hash_code_key].append([data['index'][j], data['label'][j].item()])\n",
    "        \n",
    "    hash_dict_list.append(hash_dict)\n",
    "    print(len(hash_dict.keys()))\n",
    "    # if batch_idx == 10:\n",
    "    #     break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of buckets in each hash table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys in hash_dict is: 7074\n",
      "The number of keys in hash_dict is: 11028\n",
      "The number of keys in hash_dict is: 14356\n",
      "The number of keys in hash_dict is: 8898\n",
      "The number of keys in hash_dict is: 11214\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(hash_dict_list)):\n",
    "    # Count the number of keys in the hash_dict\n",
    "    hash_dict_key_count = len(hash_dict_list[i].keys())\n",
    "    # Print the count\n",
    "    print(f\"The number of keys in hash_dict is: {hash_dict_key_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash codes for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Batches: 100%|██████████| 782/782 [00:15<00:00, 50.79it/s]\n",
      "Processing Test Batches: 100%|██████████| 782/782 [00:13<00:00, 56.68it/s]\n",
      "Processing Test Batches: 100%|██████████| 782/782 [00:14<00:00, 55.82it/s]\n",
      "Processing Test Batches: 100%|██████████| 782/782 [00:14<00:00, 55.10it/s]\n",
      "Processing Test Batches: 100%|██████████| 782/782 [00:13<00:00, 58.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store similar images for each hyperplane\n",
    "similar_images_list = []\n",
    "\n",
    "# Iterate through each hyperplane\n",
    "for i in range(len(hyperplanes)):\n",
    "    # Create a dictionary to store similar images for this hyperplane\n",
    "    similar_images = {}\n",
    "\n",
    "    # Process test data\n",
    "    for batch_idx, data in enumerate(tqdm(test_dataloader, desc=\"Processing Test Batches\")):\n",
    "        feature = data['features'].to(device)\n",
    "\n",
    "        # Compute hash values for the test data\n",
    "        hash_value = (feature @ hyperplanes[i])\n",
    "        test_hash_code = torch.where(hash_value > 0, 1, 0)\n",
    "\n",
    "        # Convert the hash code tensor to a list of tuples for dictionary key compatibility\n",
    "        test_hash_code_list = test_hash_code.tolist()\n",
    "\n",
    "        # Iterate through each test image\n",
    "        for j in range(len(test_hash_code_list)):\n",
    "            # Create a unique key from (batch_idx, i, label)\n",
    "            test_key = (data['index'][j], data['label'][j].item())  # Convert tensor to tuple\n",
    "\n",
    "            # Create a unique key from the test hash code\n",
    "            test_hash_code_key = tuple(test_hash_code_list[j])\n",
    "\n",
    "            # Check if this hash code exists in the training hash dictionary\n",
    "            if test_hash_code_key in hash_dict_list[i]:\n",
    "                # Retrieve the corresponding images and their labels from the training hash dictionary\n",
    "                similar_images[test_key] = hash_dict_list[i][test_hash_code_key]\n",
    "            else:\n",
    "                similar_images[test_key] = []\n",
    "\n",
    "    # Store the similar images found for this hyperplane\n",
    "    similar_images_list.append(similar_images)\n",
    "\n",
    "# Now similar_images_dict contains keys as (batch_idx, i, label) and values as similar images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_images_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create union of all hash list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:18<00:00, 2752.83it/s]\n",
      "100%|██████████| 50000/50000 [00:05<00:00, 9953.23it/s] \n",
      "100%|██████████| 50000/50000 [00:01<00:00, 25852.06it/s]\n",
      "100%|██████████| 50000/50000 [00:02<00:00, 19528.40it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 15770.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary to store similar images\n",
    "similar_images_dict = {}\n",
    "\n",
    "# Iterate through the similar_images_dict to populate similar_images_list\n",
    "for i in similar_images_list:\n",
    "    for j in tqdm(i.keys()):\n",
    "        # Check if the key already exists in similar_images_list\n",
    "        if j not in similar_images_list:\n",
    "            similar_images_dict[j] = []  # Initialize the list if it doesn't exist\n",
    "\n",
    "        similar_images_dict[j].extend(i[j])  # Deep copy of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [1:36:55<00:00,  8.60it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the dictionary to store similar images\n",
    "unique_images_dict = {}\n",
    "\n",
    "# Iterate through the similar_images_dict to populate unique_images_dict\n",
    "for i, j in tqdm(similar_images_dict.items()):\n",
    "    if i not in unique_images_dict:\n",
    "        unique_images_dict[i] = []  # Initialize the list only if the key doesn't exist\n",
    "\n",
    "    for k in j:\n",
    "        if k not in unique_images_dict[i]:  # Check if 'k' is not already in the list\n",
    "            unique_images_dict[i].append(k)  # Append 'k' to the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing keys:  19%|█▉        | 9480/50000 [07:21<31:25, 21.49it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Process each similar image for the current key with tqdm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(similar_images[:\u001b[38;5;241m100\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing similar images for key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Assuming image is a list/tuple with at least four elements\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     train_feature \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ds_feature[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][image[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Calculate cosine similarity and store it\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     similarity_score \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(test_feature, train_feature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2742\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/arrow_dataset.py:2727\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2725\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2726\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2727\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2728\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2729\u001b[0m )\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/formatting.py:647\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    646\u001b[0m     pa_table_to_format \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mdrop(col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m pa_table\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m format_columns)\n\u001b[0;32m--> 647\u001b[0m     formatted_output \u001b[38;5;241m=\u001b[39m formatter(pa_table_to_format, query_type\u001b[38;5;241m=\u001b[39mquery_type)\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_all_columns:\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_output, MutableMapping):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/formatting.py:403\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/formatting.py:443\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 443\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[1;32m    444\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/formatting/formatting.py:145\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(pa_table\u001b[38;5;241m.\u001b[39mto_pydict())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process each key in similar_images_dict with tqdm\n",
    "for key, similar_images in tqdm(unique_images_dict.items(), desc=\"Processing keys\"):\n",
    "    \n",
    "    test_feature = torch.tensor(ds_feature['test'][key[0]]['features'])\n",
    "    # Process each similar image for the current key with tqdm\n",
    "    for i, image in tqdm(enumerate(similar_images[:100]), desc=f\"Processing similar images for key {key[0]}\", leave=False):\n",
    "        # Assuming image is a list/tuple with at least four elements\n",
    "        train_feature = torch.tensor(ds_feature['train'][image[0]]['features'])\n",
    "        \n",
    "        # Calculate cosine similarity and store it\n",
    "        similarity_score = F.cosine_similarity(test_feature, train_feature, dim=0)\n",
    "        image = list(image[:2])  # Make a copy to modify\n",
    "        # Append the similarity score to the copied list\n",
    "        image.append(similarity_score.item())\n",
    "        \n",
    "        # Update the original list in similar_images\n",
    "        similar_images[i] = image[:3]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_images_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating metrics: 10000it [00:01, 5377.86it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP): 0.5984044666674743\n",
      "Precision@10: 0.45985\n",
      "Precision@50: 0.2562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define functions to calculate Average Precision (AP) and Precision@K\n",
    "def average_precision(predictions, targets):\n",
    "    relevant_indices = (targets == 1).nonzero(as_tuple=True)[0]\n",
    "    if len(relevant_indices) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precisions = []\n",
    "    for i, idx in enumerate(relevant_indices, start=1):\n",
    "        precision_at_i = (targets[:idx + 1].sum() / (idx + 1)).item()\n",
    "        precisions.append(precision_at_i)\n",
    "\n",
    "    return sum(precisions) / len(precisions)\n",
    "\n",
    "def precision_at_k(predictions, targets, k):\n",
    "    top_k_indices = torch.argsort(predictions, descending=True)[:k]\n",
    "    top_k_relevant = targets[top_k_indices].sum().item()\n",
    "    return top_k_relevant / k\n",
    "\n",
    "# Prepare lists to store metric scores\n",
    "map_scores = []\n",
    "precision_10_scores = []\n",
    "precision_50_scores = []\n",
    "\n",
    "# Example: Iterate over your dataset to calculate metrics\n",
    "for key, value in tqdm(unique_images_dict.items(), desc=\"Calculating metrics\", total=len(similar_images_list)):\n",
    "    if len(value) > 0:\n",
    "        # Sort the values by similarity score (assuming v[4] is the similarity score)\n",
    "        sorted_value = sorted(value, key=lambda x: x[-1], reverse=True)[:50]\n",
    "        predictions = torch.tensor([v[2] for v in sorted_value])  # Similarity scores\n",
    "        targets = torch.tensor([v[1] == key[1] for v in sorted_value], dtype=torch.float32)  # Relevance labels\n",
    "        \n",
    "        # Calculate Mean Average Precision for the current sample\n",
    "        ap = average_precision(predictions, targets)\n",
    "        map_scores.append(ap)\n",
    "        \n",
    "        # Calculate Precision@10 and Precision@50\n",
    "        p10 = precision_at_k(predictions, targets, k=10)\n",
    "        p50 = precision_at_k(predictions, targets, k=50)\n",
    "        precision_10_scores.append(p10)\n",
    "        precision_50_scores.append(p50)\n",
    "\n",
    "# Calculate final average metrics\n",
    "mean_avg_precision = sum(map_scores) / len(map_scores) if map_scores else 0.0\n",
    "precision_10 = sum(precision_10_scores) / len(precision_10_scores) if precision_10_scores else 0.0\n",
    "precision_50 = sum(precision_50_scores) / len(precision_50_scores) if precision_50_scores else 0.0\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Average Precision (mAP):\", mean_avg_precision)\n",
    "print(\"Precision@10:\", precision_10)\n",
    "print(\"Precision@50:\", precision_50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
